---
global:
  extraArgs:
    - "-log.level=debug"

autoscale: &autoscale
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 75
    targetMemoryUtilizationPercentage: 85
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
    behavior:
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 10
            periodSeconds: 60
        selectPolicy: Min
      scaleUp:
        stabilizationWindowSeconds: 60
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
          - type: Pods
            value: 1
            periodSeconds: 60
        selectPolicy: Min


gateway:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      memory: 128Mi
  service:
    type: ClusterIP
    port: 80
  # Add startup probe for faster readiness
  readinessProbe:
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1


# Service Account configuration to match SCC
serviceAccount:
  create: false
  name: loki

loki:
  commonConfig:
    replication_factor: 1
  memberlist:
    join_members: []
    dead_node_reclaim_time: 0s
    gossip_interval: 2s
    push_pull_interval: 10s
    gossip_nodes: 2
    gossip_to_dead_nodes_time: 15s
    left_ingesters_timeout: 30s
  storage:
    bucketNames:
      chunks: loki-data
      admin: loki-data
    s3:
      endpoint: http://minio:9000
      region: us-east-1
      s3ForcePathStyle: true
      insecure: true
  storage_config:
    aws:
      s3: s3://loki-chunks
      s3forcepathstyle: true
      bucketnames: loki-data
      region: us-east-1
      endpoint: minio:9000
      insecure: true
  limits_config:
    retention_period: 24h     # Reduce from 744h for development
    ingestion_rate_mb: 5      # Reduce from 10 for development
    ingestion_burst_size_mb: 10  # Reduce from 20
    max_streams_per_user: 0
    max_line_size: 3145728
    per_stream_rate_limit: 20M
    per_stream_rate_limit_burst: 50M
    reject_old_samples: true
    reject_old_samples_max_age: 6h  # Reduce from 168h
    discover_service_name: []
    discover_log_levels: false
    volume_enabled: true
    max_global_streams_per_user: 10000  # Reduce from 50000
    max_entries_limit_per_query: 100000
    increment_duplicate_timestamp: true
    allow_structured_metadata: true
  runtimeConfig:
    configs:
      kubearchive:
        log_push_request: true
        log_push_request_streams: true
        log_stream_creation: false
        log_duplicate_stream_info: true
  ingester:
    chunk_encoding: snappy
    chunk_target_size: 3145728
    chunk_idle_period: 5m
    max_chunk_age: 2h
    chunk_retain_period: 1h
    flush_op_timeout: 10m
  server:
    grpc_server_max_recv_msg_size: 15728640  # 15MB
    grpc_server_max_send_msg_size: 15728640
  ingester_client:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  query_scheduler:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  querier:
    # Reduce concurrency for development
    max_concurrent: 2  # Reduce from 4
  pattern_ingester:
    enabled: false

# Distributed components configuration
ingester:
  replicas: 3
  maxUnavailable: 1
  <<: *autoscale
  zoneAwareReplication:
    enabled: false
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 256Mi  # Increase from 128Mi to prevent OOM
    limits:
      memory: 512Mi  # Increase from 256Mi to prevent OOM
  persistence:
    enabled: false  # Disable persistence for dev to avoid storage class issues
    size: 10Gi
  affinity: {}
  podAntiAffinity:
    soft: {}
    hard: {}

querier:
  replicas: 3
  maxUnavailable: 1
  <<: *autoscale
  resources:
    requests:
      cpu: 50m  # Keep at 50m
      memory: 128Mi  # Increase from 128Mi for safety
    limits:
      memory: 512Mi  # Restore to 512Mi as it needs more memory
  affinity: {}

queryFrontend:
  replicas: 1
  resources:
    requests:
      cpu: 25m  # Keep CPU low
      memory: 128Mi  # Increase from 64Mi
    limits:
      memory: 256Mi  # Increase from 128Mi to 256Mi

queryScheduler:
  replicas: 1
  resources:
    requests:
      cpu: 25m  # Reduce from 50m
      memory: 32Mi  # Reduce from 64Mi
    limits:
      memory: 64Mi  # Reduce from 128Mi

distributor:
  replicas: 3
  maxUnavailable: 1
  <<: *autoscale
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 128Mi  # Reduce from 256Mi
    limits:
      memory: 256Mi  # Reduce from 512Mi
  affinity: {}

compactor:
  replicas: 1
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 64Mi  # Reduce from 128Mi
    limits:
      memory: 128Mi  # Reduce from 256Mi

indexGateway:
  replicas: 1
  maxUnavailable: 0
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 128Mi  # Reduce from 256Mi
    limits:
      memory: 256Mi  # Reduce from 512Mi
  affinity: {}

# Disable all cache components to avoid Pending issues in development
chunksCache:
  enabled: false
  replicas: 0

resultsCache:
  enabled: false
  replicas: 0

memcached:
  enabled: true
  replicas: 1
  maxItemMemory: 10

memcachedResults:
  enabled: true
  replicas: 1
  maxItemMemory: 10

memcachedChunks:
  enabled: true
  replicas: 1
  maxItemMemory: 10

memcachedFrontend:
  enabled: true
  replicas: 1
  maxItemMemory: 10

memcachedIndexQueries:
  enabled: true
  replicas: 1
  maxItemMemory: 10

memcachedIndexWrites:
  enabled: true
  replicas: 1
  maxItemMemory: 10
