---
global:
  extraArgs:
    - "-log.level=debug"

gateway:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      memory: 128Mi
  service:
    type: ClusterIP
    port: 80
  # Add startup probe for faster readiness
  readinessProbe:
    initialDelaySeconds: 5   # Reduce from default 15
    periodSeconds: 5         # Check more frequently
    timeoutSeconds: 3
    failureThreshold: 3
    successThreshold: 1


# Service Account configuration to match SCC
serviceAccount:
  create: false
  name: loki

loki:
  commonConfig:
    replication_factor: 1
  memberlist:
    join_members: []
    # How long to wait before reclaiming a dead node's tokens
    # Reduced to 2 minutes for development (faster cleanup with single replica)
    # This helps remove stale ring instances quickly when pods are restarted
    dead_node_reclaim_time: 2m
    # How often to gossip with other nodes (lower = faster detection of failures)
    # Keep at 2s for quick failure detection
    gossip_interval: 2s
    # How often to do full state sync with other nodes
    # Reduced for development to sync faster
    push_pull_interval: 5s
    # Number of random nodes to gossip with per interval
    # Set to 1 for development (only 1 ingester replica)
    gossip_nodes: 1
    # How long to continue gossiping to dead nodes (helps propagate death info)
    # Reduced for development to propagate death info faster
    gossip_to_dead_nodes_time: 10s
    # How long to wait for an ingester to gracefully leave before considering it dead
    # This should be longer than terminationGracePeriodSeconds to allow graceful shutdown
    # Reduced to 60s for development (faster cleanup)
    left_ingesters_timeout: 60s
    max_join_backoff: 1m
    max_join_retries: 10
    min_join_backoff: 1s
    rejoin_interval: 90s
  storage:
    bucketNames:
      chunks: loki-data
      admin: loki-data
    s3:
      endpoint: http://minio:9000
      region: us-east-1
      s3ForcePathStyle: true
      insecure: true
  storage_config:
    aws:
      s3: s3://loki-chunks
      s3forcepathstyle: true
      bucketnames: loki-data
      region: us-east-1
      endpoint: minio:9000
      insecure: true
  limits_config:
    shard_streams:
      enabled: false
    retention_period: 24h     # Reduce from 744h for development
    ingestion_rate_mb: 5      # Reduce from 10 for development
    ingestion_burst_size_mb: 10  # Reduce from 20
    max_streams_per_user: 0
    max_line_size: 3145728
    per_stream_rate_limit: 20M
    per_stream_rate_limit_burst: 50M
    reject_old_samples: true
    reject_old_samples_max_age: 6h  # Reduce from 168h
    discover_service_name: []
    discover_log_levels: false
    volume_enabled: true
    max_global_streams_per_user: 10000  # Reduce from 50000
    max_entries_limit_per_query: 100000
    increment_duplicate_timestamp: true
    allow_structured_metadata: true
    # Query timeout configuration to prevent context canceled errors
    max_query_length: 721h      # Maximum time range for queries (30 days)
    max_query_parallelism: 32  # Maximum number of parallel queries
    query_timeout: 10m         # Timeout for individual queries (10 minutes)
  runtimeConfig:
    configs:
      kubearchive:
        log_push_request: true
        log_push_request_streams: true
        log_stream_creation: false
        log_duplicate_stream_info: true
  ingester:
    autoforget_unhealthy: true
    chunk_encoding: snappy
    chunk_target_size: 3145728
    chunk_idle_period: 5m
    max_chunk_age: 2h
    chunk_retain_period: 1h
    flush_op_timeout: 10m
  server:
    grpc_server_max_recv_msg_size: 15728640  # 15MB
    grpc_server_max_send_msg_size: 15728640
  ingester_client:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  query_scheduler:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  querier:
    # Reduce concurrency for development
    max_concurrent: 2  # Reduce from 4
  query_range:
    # Query range configuration for better query performance
    parallelise_shardable_queries: true  # Enable parallel query execution
    align_queries_with_step: true         # Align queries with step for better caching
  pattern_ingester:
    enabled: false

# Distributed components configuration
ingester:
  replicas: 1  # Reduce from 3 to 1 for development
  autoscaling:
    enabled: false  # Disable autoscaling for dev
  zoneAwareReplication:
    enabled: false
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 256Mi  # Increase from 128Mi to prevent OOM
    limits:
      memory: 512Mi  # Increase from 256Mi to prevent OOM
  persistence:
    enabled: false  # Disable persistence for dev to avoid storage class issues
    size: 10Gi
  affinity: {}
  podAntiAffinity:
    soft: {}
    hard: {}
  # AWS credentials for S3/MinIO access
  # These match the MinIO credentials from loki-helm-minio-values.yaml
  # Using secretKeyRef to satisfy kube-linter security requirements
  extraEnv:
    - name: AWS_ACCESS_KEY_ID
      valueFrom:
        secretKeyRef:
          name: kubearchive-loki
          key: AWS_ACCESS_KEY_ID
    - name: AWS_SECRET_ACCESS_KEY
      valueFrom:
        secretKeyRef:
          name: kubearchive-loki
          key: AWS_SECRET_ACCESS_KEY
    - name: AWS_DEFAULT_REGION
      valueFrom:
        secretKeyRef:
          name: kubearchive-loki
          key: AWS_DEFAULT_REGION
  # Graceful shutdown configuration to prevent stale ring instances
  # Give Loki time to flush chunks and leave the ring gracefully
  # Set to 90s to be longer than left_ingesters_timeout (60s) but still allow quick cleanup
  terminationGracePeriodSeconds: 90
  lifecycle:
    preStop:
      exec:
        # Sleep to allow readiness probe to fail, removing pod from service endpoints
        # This gives distributor time to stop sending new requests before shutdown
        command:
          - /bin/sh
          - -c
          - sleep 10

querier:
  replicas: 1
  autoscaling:
    enabled: false
  resources:
    requests:
      cpu: 50m  # Keep at 50m
      memory: 128Mi  # Increase from 128Mi for safety
    limits:
      memory: 512Mi  # Restore to 512Mi as it needs more memory
  affinity: {}

queryFrontend:
  replicas: 1
  resources:
    requests:
      cpu: 25m  # Keep CPU low
      memory: 128Mi  # Increase from 64Mi
    limits:
      memory: 256Mi  # Increase from 128Mi to 256Mi

queryScheduler:
  replicas: 1
  resources:
    requests:
      cpu: 25m  # Reduce from 50m
      memory: 32Mi  # Reduce from 64Mi
    limits:
      memory: 64Mi  # Reduce from 128Mi

distributor:
  replicas: 1  # Reduce from 3 to 1 for development
  autoscaling:
    enabled: false  # Disable autoscaling for dev
  maxUnavailable: 1
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 128Mi  # Reduce from 256Mi
    limits:
      memory: 256Mi  # Reduce from 512Mi
  affinity: {}

compactor:
  replicas: 1
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 64Mi  # Reduce from 128Mi
    limits:
      memory: 128Mi  # Reduce from 256Mi

indexGateway:
  replicas: 1
  maxUnavailable: 0
  resources:
    requests:
      cpu: 50m  # Reduce from 100m
      memory: 128Mi  # Reduce from 256Mi
    limits:
      memory: 256Mi  # Reduce from 512Mi
  affinity: {}

# Cache configuration - Helm chart automatically generates query_range and chunk_store_config
# from chunksCache and resultsCache settings
# Increased maxItemMemory to prevent "object too large for cache" errors
chunksCache:
  enabled: true
  replicas: 1
  batchSize: 256  # Batch size for sending/receiving chunks from cache
  parallelism: 10  # Parallel threads for cache operations
  maxItemMemory: 10  # MB
  defaultValidity: 12h  # How long cached chunks are stored

resultsCache:
  enabled: true
  replicas: 1
  maxItemMemory: 10  # MB
  defaultValidity: 12h  # How long cached query results are stored

memcached:
  enabled: true
  maxItemMemory: 10  # MB - Shared default for general memcached instances

# Shared memcached configuration (used as defaults for all memcached instances)
# These don't deploy separate instances - they configure shared settings
memcachedResults:
  enabled: true
  maxItemMemory: 10  # MB - For query result caching

memcachedChunks:
  enabled: true
  maxItemMemory: 10  # MB - For chunk caching

memcachedFrontend:
  enabled: true
  maxItemMemory: 10  # MB - Frontend cache can handle large query results

memcachedIndexQueries:
  enabled: true
  maxItemMemory: 10  # MB - Index queries can be large

memcachedIndexWrites:
  enabled: true
  maxItemMemory: 10  # MB - Index write operations
