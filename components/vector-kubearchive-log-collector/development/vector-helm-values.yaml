---
role: Agent

customConfig:
  data_dir: /vector-data-dir
  api:
    enabled: true
    address: 127.0.0.1:8686
    playground: false
  sources:
    k8s_logs:
      type: kubernetes_logs
      rotate_wait_secs: 5
      glob_minimum_cooldown_ms: 500
      max_line_bytes: 3145728
      auto_partial_merge: true
    metrics:
      type: internal_metrics
  transforms:
    reduce_events:
      type: reduce
      inputs:
        - k8s_logs
      group_by:
        - file
      max_events: 100
      expire_after_ms: 10000
      merge_strategies:
        message: concat_newline
    remap_app_logs:
      type: remap
      inputs:
        - reduce_events
      source: |-
        .tmp = del(.)
        # Preserve original kubernetes fields for Loki labels
        if exists(.tmp.kubernetes.pod_uid) {
          .pod_id = del(.tmp.kubernetes.pod_uid)
        } else {
          .pod_id = "unknown_pod_id"
        }
        if exists(.tmp.kubernetes.container_name) {
          .container = del(.tmp.kubernetes.container_name)
        } else {
          .container = "unknown_container"
        }
        # Extract namespace for low cardinality labeling
        if exists(.tmp.kubernetes.pod_namespace) {
          .namespace = del(.tmp.kubernetes.pod_namespace)
        } else {
          .namespace = "unknown_namespace"
        }
        # General message field handling
        if exists(.tmp.message) {
          .message = to_string(del(.tmp.message)) ?? "no_message"
        } else {
          .message = "no_message"
        }
        # Basic data sanitization to prevent 400 errors
        # Truncate very long messages
        if length(.message) > 1048576 {
          .message = slice!(.message, 0, 1048576) + "...[TRUNCATED]"
        }
        # Clean up temporary fields
        del(.tmp)
  sinks:
    loki:
      type: loki
      inputs: ["remap_app_logs"]
      # Send to Loki gateway
      endpoint: "http://loki-gateway.product-kubearchive-logging.svc.cluster.local:80"
      encoding:
        codec: "text"  # Use text instead of json to avoid metadata issues
        except_fields: ["tmp"]  # Exclude temporary fields
        only_fields:
          - message
      structured_metadata:
        pod_id: "{{`{{ pod_id }}`}}"
        container: "{{`{{ container }}`}}"
      auth:
        strategy: "basic"
        user: "${LOKI_USERNAME}"
        password: "${LOKI_PASSWORD}"
      tenant_id: "kubearchive"
      request:
        headers:
          X-Scope-OrgID: kubearchive
        timeout_secs: 60  # Shorter timeout
      batch:
        max_bytes: 2097152  # Reduce to 2MB (half of Loki's 4MB/sec limit)
        max_events: 1000   # Limit number of events per batch
        timeout_secs: 10   # Send batches more frequently
      compression: "gzip"  # Enable compression to reduce data size
      labels:
        stream: "{{`{{ namespace }}`}}"
      buffer:
        type: "memory"
        max_events: 10000
        when_full: "drop_newest"  # Drop newest instead of blocking
    prometheus_sink:
      type: prometheus_exporter
      inputs:
        - metrics
env:
  - name: LOKI_USERNAME
    valueFrom:
      secretKeyRef:
        name: kubearchive-loki
        key: USERNAME
  - name: LOKI_PASSWORD
    valueFrom:
      secretKeyRef:
        name: kubearchive-loki
        key: PASSWORD
affinity:
    nodeAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 1
        preference:
          matchExpressions:
          - key: konflux-ci.dev/workload
            operator: In
            values:
              - konflux-tenants
tolerations:
  # Tolerate standard Kubernetes node taints to allow scheduling on temporarily not-ready nodes
  # This is important for DaemonSets that should run on all nodes
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300  # Wait 5 minutes before evicting from not-ready nodes
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300  # Wait 5 minutes before evicting from unreachable nodes
  # Tolerate node pressure conditions to prevent eviction during temporary resource pressure
  # These allow pods to stay on nodes during brief pressure events
  - effect: NoSchedule
    key: node.kubernetes.io/memory-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/disk-pressure
    operator: Exists
  - effect: NoSchedule
    key: node.kubernetes.io/pid-pressure
    operator: Exists
image:
  repository: quay.io/kubearchive/vector
  tag: 0.46.1-distroless-libc
serviceAccount:
  create: true
  name: vector
securityContext:
  allowPrivilegeEscalation: false
  runAsUser: 0
  capabilities:
    drop:
    - CHOWN
    - DAC_OVERRIDE
    - FOWNER
    - FSETID
    - KILL
    - NET_BIND_SERVICE
    - SETGID
    - SETPCAP
    - SETUID
  readOnlyRootFilesystem: true
  seLinuxOptions:
    type: spc_t
  seccompProfile:
    type: RuntimeDefault

# Override default volumes to be more specific and secure
extraVolumes:
  - name: varlog
    hostPath:
      path: /var/log/pods
      type: Directory
  - name: varlibdockercontainers
    hostPath:
      path: /var/lib/containers
      type: DirectoryOrCreate

extraVolumeMounts:
  - name: varlog
    mountPath: /var/log/pods
    readOnly: true
  - name: varlibdockercontainers
    mountPath: /var/lib/containers
    readOnly: true

# Configure Vector to use emptyDir for its default data volume instead of hostPath
persistence:
  enabled: false
