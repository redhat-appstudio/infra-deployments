---
global:
  extraArgs:
    - "-log.level=debug"

autoscale: &autoscale
  autoscaling:
    enabled: true
    minReplicas: 1
    maxReplicas: 3
    targetCPUUtilizationPercentage: 75
    targetMemoryUtilizationPercentage: 85
    podDisruptionBudget:
      enabled: true
      minAvailable: 1
    behavior:
      enabled: true
      scaleDown:
        stabilizationWindowSeconds: 300
        policies:
          - type: Percent
            value: 10
            periodSeconds: 60
        selectPolicy: Min
      scaleUp:
        stabilizationWindowSeconds: 60
        policies:
          - type: Percent
            value: 50
            periodSeconds: 60
          - type: Pods
            value: 1
            periodSeconds: 60
        selectPolicy: Min


gateway:
  service:
    type: LoadBalancer
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      memory: 256Mi

# Basic Loki configuration with S3 storage
loki:
  commonConfig:
    replication_factor: 3
  memberlist:
    join_members: []
    # How long to wait before reclaiming a dead node's tokens
    # Reduced to 2 minutes for development (faster cleanup with single replica)
    # This helps remove stale ring instances quickly when pods are restarted
    dead_node_reclaim_time: 2m
    # How often to gossip with other nodes (lower = faster detection of failures)
    # Keep at 2s for quick failure detection
    gossip_interval: 2s
    # How often to do full state sync with other nodes
    # Reduced for development to sync faster
    push_pull_interval: 5s
    # Number of random nodes to gossip with per interval
    # Set to 1 for development (only 1 ingester replica)
    gossip_nodes: 1
    # How long to continue gossiping to dead nodes (helps propagate death info)
    # Reduced for development to propagate death info faster
    gossip_to_dead_nodes_time: 10s
    # How long to wait for an ingester to gracefully leave before considering it dead
    # This should be longer than terminationGracePeriodSeconds to allow graceful shutdown
    # Reduced to 60s for development (faster cleanup)
    left_ingesters_timeout: 60s
    max_join_backoff: 1m
    max_join_retries: 10
    min_join_backoff: 1s
    rejoin_interval: 90s
  storage:
    type: s3
    # bucketNames: Fill it on the generator for each cluster
    s3:
      region: us-east-1
  storage_config:
    aws:
      # bucketnames: Fill it on the generator for each cluster
      region: us-east-1
      s3forcepathstyle: false
  # Configure ingestion limits to handle Vector's data volume
  limits_config:
      retention_period: 744h  # 31 days retention
      ingestion_rate_mb: 20
      ingestion_burst_size_mb: 40
      ingestion_rate_strategy: "local"
      max_streams_per_user: 0
      max_line_size: 2097152
      per_stream_rate_limit: 20M
      per_stream_rate_limit_burst: 50M
      reject_old_samples: false
      reject_old_samples_max_age: 168h
      discover_service_name: []
      discover_log_levels: false
      volume_enabled: true
      max_global_streams_per_user: 50000
      max_entries_limit_per_query: 100000
      increment_duplicate_timestamp: true
      allow_structured_metadata: true
  runtimeConfig:
    configs:
      kubearchive:
        log_push_request: true
        log_push_request_streams: true
        log_stream_creation: false
        log_duplicate_stream_info: true
  server:
    grpc_server_max_recv_msg_size: 15728640  # 15MB
    grpc_server_max_send_msg_size: 15728640
  ingester_client:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  query_scheduler:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  ingester:
    autoforget_unhealthy: true
    chunk_target_size: 4194304  # 4MB
    chunk_idle_period: 5m
    max_chunk_age: 2h
    chunk_retain_period: 1h
    chunk_encoding: snappy
    flush_op_timeout: 10m
  # Tuning for high-load queries
  querier:
    max_concurrent: 8
  query_range:
    # split_queries_by_interval deprecated in Loki 3.x - removed
    parallelise_shardable_queries: true

# Distributed components configuration
ingester:
  replicas: 3
  autoscaling:
    enabled: true
    <<: *autoscale
  zoneAwareReplication:
    enabled: true
  maxUnavailable: 1
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 2000m
      memory: 2Gi
  persistence:
    enabled: true
    size: 10Gi
  affinity: {}
  podAntiAffinity:
    soft: {}
    hard: {}

querier:
  replicas: 3
  autoscaling:
    enabled: true
    <<: *autoscale
  maxUnavailable: 1
  resources:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      memory: 1Gi
  affinity: {}

queryFrontend:
  replicas: 2
  maxUnavailable: 1
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      memory: 512Mi

queryScheduler:
  replicas: 2
  maxUnavailable: 1
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      memory: 512Mi

distributor:
  replicas: 3
  autoscaling:
    enabled: true
    <<: *autoscale
  maxUnavailable: 1
  resources:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      memory: 1Gi
  affinity: {}

compactor:
  replicas: 1
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      memory: 1Gi

indexGateway:
  replicas: 2
  maxUnavailable: 0
  resources:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      memory: 1Gi
  affinity: {}

# Enable Memcached caches for performance
chunksCache:
  enabled: true
  replicas: 1
  maxItemMemory: 10  # MB

resultsCache:
  enabled: true
  replicas: 1
  maxItemMemory: 10  # MB

memcached:
  enabled: true
  maxItemMemory: 10  # MB

memcachedResults:
  enabled: true
  maxItemMemory: 10  # MB

memcachedChunks:
  enabled: true
  maxItemMemory: 10  # MB

memcachedFrontend:
  enabled: true
  maxItemMemory: 10  # MB

memcachedIndexQueries:
  enabled: true
  maxItemMemory: 10  # MB

memcachedIndexWrites:
  enabled: true
  maxItemMemory: 10  # MB

# Disable Minio - staging uses S3 with IAM role
minio:
  enabled: false

# Resources for memcached exporter to satisfy linter
memcachedExporter:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      memory: 128Mi
