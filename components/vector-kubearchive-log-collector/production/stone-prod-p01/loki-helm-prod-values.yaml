---
global:
  extraArgs:
    - "-log.level=debug"

gateway:
  service:
    type: LoadBalancer
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      memory: 256Mi

# Basic Loki configuration with S3 storage
loki:
  commonConfig:
    replication_factor: 3
  memberlist:
    join_members: []
    # How long to wait before reclaiming a dead node's tokens
    # Set to 5 minutes to allow graceful shutdowns but remove stale instances quickly
    dead_node_reclaim_time: 5m
    # How often to gossip with other nodes (lower = faster detection of failures)
    gossip_interval: 2s
    # How often to do full state sync with other nodes
    push_pull_interval: 10s
    # Number of random nodes to gossip with per interval
    gossip_nodes: 3
    # How long to continue gossiping to dead nodes (helps propagate death info)
    gossip_to_dead_nodes_time: 15s
    # How long to wait for an ingester to gracefully leave before considering it dead
    # This should be longer than terminationGracePeriodSeconds to allow graceful shutdown
    left_ingesters_timeout: 90s
  # Required storage configuration for Helm chart
  storage:
    type: s3
    # bucketNames: Fill it on the generator for each cluster
    s3:
      region: us-east-1
  storage_config:
    aws:
      # bucketnames: Fill it on the generator for each cluster
      region: us-east-1
      s3forcepathstyle: false
  # Configure ingestion limits to handle Vector's data volume
  limits_config:
    shard_streams:
      enabled: false
    retention_period: 744h  # 31 days retention
    ingestion_rate_mb: 100
    ingestion_burst_size_mb: 300
    ingestion_rate_strategy: "local"
    max_streams_per_user: 0
    max_line_size: 2097152
    per_stream_rate_limit: 100M
    per_stream_rate_limit_burst: 400M
    reject_old_samples: false
    reject_old_samples_max_age: 168h
    discover_service_name: []
    discover_log_levels: false
    volume_enabled: true
    max_global_streams_per_user: 75000
    max_entries_limit_per_query: 100000
    increment_duplicate_timestamp: true
    allow_structured_metadata: true
  runtimeConfig:
    configs:
      kubearchive:
        log_push_request: true
        log_push_request_streams: true
        log_stream_creation: true
        log_duplicate_stream_info: true
  ingester:
    autoforget_unhealthy: true
    chunk_target_size: 8388608        # 8MB
    chunk_idle_period: 5m
    max_chunk_age: 2h
    chunk_encoding: snappy            # Compress data (reduces S3 transfer size)
    chunk_retain_period: 1h           # Keep chunks in memory after flush
    flush_op_timeout: 10m             # Add timeout for S3 operations
  server:
    grpc_server_max_recv_msg_size: 15728640  # 15MB
    grpc_server_max_send_msg_size: 15728640
  ingester_client:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  query_scheduler:
    grpc_client_config:
      max_recv_msg_size: 15728640  # 15MB
      max_send_msg_size: 15728640  # 15MB
  # Tuning for high-load queries
  querier:
    max_concurrent: 8
  query_range:
    # split_queries_by_interval deprecated in Loki 3.x - removed
    parallelise_shardable_queries: false

# Distributed components configuration
ingester:
  replicas: 3
  autoscaling:
    enabled: true
  zoneAwareReplication:
    enabled: true
  maxUnavailable: 1
  resources:
    requests:
      cpu: 500m
      memory: 4Gi
    limits:
      cpu: 2000m
      memory: 6Gi
  persistence:
    enabled: true
    size: 10Gi
  affinity: {}
  podAntiAffinity:
    soft: {}
    hard: {}
  # Graceful shutdown configuration to prevent stale ring instances
  # Give Loki time to flush chunks and leave the ring gracefully
  terminationGracePeriodSeconds: 120  # 2 minutes - longer than left_ingesters_timeout
  lifecycle:
    preStop:
      exec:
        # Sleep to allow readiness probe to fail, removing pod from service endpoints
        # This gives distributor time to stop sending new requests before shutdown
        command:
          - /bin/sh
          - -c
          - sleep 10

querier:
  replicas: 3
  autoscaling:
    enabled: true
  maxUnavailable: 1
  resources:
    requests:
      cpu: 500m
      memory: 2Gi
    limits:
      memory: 4Gi
  affinity: {}

queryFrontend:
  replicas: 2
  maxUnavailable: 1
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      memory: 1Gi

queryScheduler:
  replicas: 2
  maxUnavailable: 1
  resources:
    requests:
      cpu: 200m
      memory: 256Mi
    limits:
      memory: 512Mi

distributor:
  replicas: 5
  autoscaling:
    enabled: true
    minReplicas: 5
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
  maxUnavailable: 1
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      memory: 2Gi
  affinity: {}

compactor:
  replicas: 1
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
  resources:
    requests:
      cpu: 200m
      memory: 512Mi
    limits:
      memory: 1Gi

indexGateway:
  replicas: 2
  maxUnavailable: 0
  resources:
    requests:
      cpu: 300m
      memory: 512Mi
    limits:
      memory: 1Gi
  affinity: {}

# Cache configuration - Helm chart automatically generates query_range and chunk_store_config
# from chunksCache and resultsCache settings
# Increased maxItemMemory to prevent "object too large for cache" errors
chunksCache:
  enabled: true
  replicas: 1
  batchSize: 256  # Batch size for sending/receiving chunks from cache
  parallelism: 10  # Parallel threads for cache operations
  maxItemMemory: 30  # MB - Increased from 10MB to handle chunks (3MB target + compression overhead + metadata)
  defaultValidity: 12h  # How long cached chunks are stored

resultsCache:
  enabled: true
  replicas: 1
  maxItemMemory: 100  # MB - Increased from 10MB to handle large query results (can be much larger than chunks)
  defaultValidity: 12h  # How long cached query results are stored

memcached:
  enabled: true
  maxItemMemory: 30  # MB - Shared default for general memcached instances

# Shared memcached configuration (used as defaults for all memcached instances)
# These don't deploy separate instances - they configure shared settings
memcachedResults:
  enabled: true
  maxItemMemory: 100  # MB - For query result caching

memcachedChunks:
  enabled: true
  maxItemMemory: 30  # MB - For chunk caching

memcachedFrontend:
  enabled: true
  maxItemMemory: 100  # MB - Frontend cache can handle large query results

memcachedIndexQueries:
  enabled: true
  maxItemMemory: 50  # MB - Index queries can be large

memcachedIndexWrites:
  enabled: true
  maxItemMemory: 30  # MB - Index write operations

# Disable Minio
minio:
  enabled: false

# Resources for memcached exporter to satisfy linter
memcachedExporter:
  resources:
    requests:
      cpu: 50m
      memory: 64Mi
    limits:
      memory: 128Mi
