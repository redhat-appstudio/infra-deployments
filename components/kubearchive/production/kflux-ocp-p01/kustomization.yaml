---
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
  - ../base
  - external-secret.yaml
  - https://github.com/kubearchive/kubearchive/releases/download/v1.9.0/kubearchive.yaml?timeout=90

namespace: product-kubearchive

# Generate kubearchive-logging ConfigMap with hash for automatic restarts
# Due to quoting limitations of generators we need to introduce the values with the |
# See https://github.com/kubernetes-sigs/kustomize/issues/4845#issuecomment-1671570428
configMapGenerator:
    - name: kubearchive-logging
      literals:
          - |
              POD_ID=cel:metadata.uid
          - |
              NAMESPACE=cel:metadata.namespace
          - |
              START=cel:status.?startTime == optional.none() ? int(now()-duration('1h'))*1000000000: status.startTime
          - |
              END=cel:status.?startTime == optional.none() ? int(now()+duration('1h'))*1000000000: int(timestamp(status.startTime)+duration('6h'))*1000000000
          - |
              LOG_URL=http://loki-gateway.product-kubearchive-logging.svc.cluster.local:80/loki/api/v1/query_range?query=%7Bstream%3D%22{NAMESPACE}%22%7D%20%7C%20pod_id%20%3D%20%60{POD_ID}%60%20%7C%20container%20%3D%20%60{CONTAINER_NAME}%60&start={START}&end={END}&direction=forward
          - |
              LOG_URL_JSONPATH=$.data.result[*].values[*][1]

patches:
  - patch: |-
      $patch: delete
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kubearchive-logging
        namespace: kubearchive

  - patch: |-
      $patch: delete
      apiVersion: v1
      kind: Secret
      metadata:
        name: kubearchive-logging
        namespace: kubearchive

  - patch: |-
      apiVersion: batch/v1
      kind: Job
      metadata:
        name: kubearchive-schema-migration
        namespace: kubearchive
        annotations:
          # Needed if just the command is changed, otherwise the job needs to be deleted manually
          argocd.argoproj.io/sync-options: Force=true,Replace=true
          ignore-check.kube-linter.io/no-read-only-root-fs: >
            "This job needs to clone a repository to do its job, so it needs write access to the FS."
      spec:
        suspend: false
        template:
          spec:
            containers:
              - name: migration
                env:
                  - name: KUBEARCHIVE_VERSION
                    value: v1.9.0
  # We don't need the Secret as it will be created by the ExternalSecrets Operator
  - patch: |-
      $patch: delete
      apiVersion: v1
      kind: Secret
      metadata:
        name: kubearchive-database-credentials
        namespace: kubearchive
  - patch: |-
      apiVersion: external-secrets.io/v1beta1
      kind: ExternalSecret
      metadata:
        name: database-secret
      spec:
        dataFrom:
          - extract:
              key: integrations-output/external-resources/appsrep09ue1/kflux-ocp-p01/kflux-ocp-p01-kube-archive-rds
  # These patches add an annotation so an OpenShift service
  # creates the TLS secrets instead of Cert Manager
  - patch: |-
      apiVersion: v1
      kind: Service
      metadata:
        name: kubearchive-api-server
        namespace: kubearchive
        annotations:
          service.beta.openshift.io/serving-cert-secret-name: kubearchive-api-server-tls
  - patch: |-
      apiVersion: v1
      kind: Service
      metadata:
        name: kubearchive-operator-webhooks
        namespace: kubearchive
        annotations:
          service.beta.openshift.io/serving-cert-secret-name: kubearchive-operator-tls
  - patch: |-
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      metadata:
        name: kubearchive-mutating-webhook-configuration
        annotations:
          service.beta.openshift.io/inject-cabundle: "true"
  - patch: |-
      apiVersion: admissionregistration.k8s.io/v1
      kind: ValidatingWebhookConfiguration
      metadata:
        name: kubearchive-validating-webhook-configuration
        annotations:
          service.beta.openshift.io/inject-cabundle: "true"
  # These patches solve Kube Linter problems
  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kubearchive-api-server
        namespace: kubearchive
      spec:
        template:
          spec:
            containers:
              - name: kubearchive-api-server
                env:
                - name: KUBEARCHIVE_OTEL_MODE
                  value: enabled
                - name: OTEL_EXPORTER_OTLP_ENDPOINT
                  value: http://otel-collector:4318
                - name: AUTH_IMPERSONATE
                  value: "true"
                securityContext:
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kubearchive-operator
        namespace: kubearchive
      spec:
        template:
          spec:
            containers:
              - name: manager
                args: [--health-probe-bind-address=:8081]
                env:
                - name: KUBEARCHIVE_OTEL_MODE
                  value: enabled
                - name: OTEL_EXPORTER_OTLP_ENDPOINT
                  value: http://otel-collector:4318
                securityContext:
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                ports:
                  - containerPort: 8081
                resources:
                  limits:
                    cpu: 100m
                    memory: 1Gi
                  requests:
                    cpu: 100m
                    memory: 1Gi

  - patch: |-
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: kubearchive-sink
        namespace: kubearchive
      spec:
        template:
          spec:
            containers:
              - name: kubearchive-sink
                env:
                - name: KUBEARCHIVE_OTEL_MODE
                  value: enabled
                - name: OTEL_EXPORTER_OTLP_ENDPOINT
                  value: http://otel-collector:4318
                securityContext:
                  readOnlyRootFilesystem: true
                  runAsNonRoot: true
                resources:
                  limits:
                    cpu: 200m
                    memory: 128Mi
                  requests:
                    cpu: 200m
                    memory: 128Mi

  # We don't need this CronJob as it is suspended, we can enable it later
  - patch: |-
      $patch: delete
      apiVersion: batch/v1
      kind: CronJob
      metadata:
        name: cluster-vacuum
        namespace: kubearchive
  # These patches remove Certificates and Issuer from Cert-Manager
  - patch: |-
      $patch: delete
      apiVersion: cert-manager.io/v1
      kind: Certificate
      metadata:
        name: "kubearchive-api-server-certificate"
        namespace: kubearchive
  - patch: |-
      $patch: delete
      apiVersion: cert-manager.io/v1
      kind: Certificate
      metadata:
        name: "kubearchive-ca"
        namespace: kubearchive
  - patch: |-
      $patch: delete
      apiVersion: cert-manager.io/v1
      kind: Issuer
      metadata:
        name: "kubearchive-ca"
        namespace: kubearchive
  - patch: |-
      $patch: delete
      apiVersion: cert-manager.io/v1
      kind: Issuer
      metadata:
        name: "kubearchive"
        namespace: kubearchive
  - patch: |-
      $patch: delete
      apiVersion: cert-manager.io/v1
      kind: Certificate
      metadata:
        name: "kubearchive-operator-certificate"
        namespace: kubearchive
